{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression - model specification\n",
    "This notebook demostrates:\n",
    "\n",
    "0. Correctly specified model (baseline)\n",
    "1. Under-specified model\n",
    "    - Uncorrelated predictors\n",
    "    - Correlated predictors\n",
    "2. Over-specified model\n",
    "    - Perfect multi-colinearity\n",
    "    - Varying degrees of Multi-colinearity\n",
    "    - Including non related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Correctly specified model (baseline)\n",
    "\n",
    "- We randomly generate 100 uncorrelated X1 and X2 following a normal distirbution.\n",
    "\n",
    "- We speficy a true regression line: $y$ = 3 + 2*$X_1$ + 4*$X_2$ + e\n",
    "\n",
    "- We correctly include X1 and X2 in our regression model: y ~ X1 + X2 (with intercept)\n",
    "\n",
    "- We fit 1000 such regression models and check for biasedness and preciseness of the estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = []\n",
    "for i in range(1000):\n",
    "    X1 = np.random.randn(100) + 2\n",
    "    X2 = np.random.randn(100) + 2\n",
    "    \n",
    "    e = np.random.randn(100)\n",
    "    y = 3 + 2*X1 + 4*X2 + e\n",
    "    X = sm.add_constant(np.column_stack([X1,X2]))\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    estimates.append(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of estimates: [3.0037811  2.00347394 3.99617821]\n",
      "SE of estimates: [0.30086517 0.09971629 0.10145132]\n"
     ]
    }
   ],
   "source": [
    "#The averages of estimates across the 1000 replications are:\n",
    "print(\"Average of estimates:\", np.mean(np.array(estimates),axis=0))\n",
    "print(\"SE of estimates:\", np.std(np.array(estimates),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Under-specified model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncorrelated predictors (#1)\n",
    "- We randomly generate 100 uncorrelated X1 and X2 following a normal distirbution.\n",
    "\n",
    "- We speficy a true regression line: $y$ = 3 + 2*$X_1$ + 4*$X_2$ + e\n",
    "\n",
    "- We only include X1 in our regression model: y ~ X1 (with intercept)\n",
    "\n",
    "- We fit 1000 such regression models and check for biasedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = []\n",
    "for i in range(1000):\n",
    "    means = [2 , 2]\n",
    "    cov = [[1,0], [0,1]]\n",
    "    \n",
    "    X = np.random.multivariate_normal(means,cov,100)\n",
    "\n",
    "    e = np.random.randn(100)\n",
    "    \n",
    "    y = 3 + 2*X[:,0] + 4*X[:,1] + e\n",
    "    \n",
    "    X = sm.add_constant(X[:,0])\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    estimates.append(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of estimates: [11.01614906  1.99128492]\n",
      "SE of estimates: [0.95786376 0.42621356]\n"
     ]
    }
   ],
   "source": [
    "#The averages of estimates across the 1000 replications are:\n",
    "print(\"Average of estimates:\", np.mean(np.array(estimates),axis=0))\n",
    "\n",
    "print(\"SE of estimates:\", np.std(np.array(estimates),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the intercept estimate is **biased** (if will be unbiased only if your omitted variable has a mean of zero), and the coefficient for b1 is **unbiased**. The SE of the estimates are much higher than that in a correctly specified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlated predictors (#2)\n",
    "- We randomly generate 100 correlated X1 and X2 following a bi-variate normal distirbution (so X1 and X2 are having a correlation coefficient of 0.4).\n",
    "\n",
    "- We speficy a true regression line: $y$ = 3 + 2*$X_1$ + 4*$X_2$ + e\n",
    "\n",
    "- We only include X1 in our regression model: y ~ X1 (with intercept)\n",
    "\n",
    "- We fit 1000 such regression models and check for biasedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = []\n",
    "for i in range(1000):\n",
    "    means = [2, 2]\n",
    "    cov = [[1,0.4], [0.4,1]]\n",
    "    X = np.random.multivariate_normal(means,cov,100)\n",
    "\n",
    "    e = np.random.randn(100)\n",
    "    y = 3 + 2*X[:,0] + 4*X[:,1] + e\n",
    "    X = sm.add_constant(X[:,0])\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    estimates.append(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.77702431, 3.60628815])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The averages of estimates across the 1000 replications are:\n",
    "np.mean(np.array(estimates),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85773046, 0.38963273])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The averages of estimates across the 1000 replications are:\n",
    "np.std(np.array(estimates),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the intercept estimate is **biased** (it will be unbiased only if both of your variable have a mean of zero), and the coefficient for b1 is also **biased** from its true value of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: when predictors are correlated, omitting one will make other estimates biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Over-specified model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perfect multicolinearity (#1)\n",
    "- We randomly generate 100 data points for X1 following a normal distribution.\n",
    "- We set X2 = 1 - X1\n",
    "\n",
    "- We speficy a true regression line: $y$ = 3 + 2*$X_1$ + 4*$X_2$ + e\n",
    "\n",
    "- We only fit the regression model as: y ~ X1 + X2 (with intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.random.randn(100)\n",
    "X2 = 1 - X1\n",
    "e = np.random.randn(100)\n",
    "\n",
    "y = 3 + 2*X1 + 4*X2 + e\n",
    "\n",
    "X = sm.add_constant(np.column_stack([X1,X2]))\n",
    "\n",
    "model = sm.OLS(y,X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.826</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.825</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   466.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 13 Feb 2024</td> <th>  Prob (F-statistic):</th> <td>4.98e-39</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:50:43</td>     <th>  Log-Likelihood:    </th> <td> -141.90</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   287.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   293.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    4.0724</td> <td>    0.075</td> <td>   54.100</td> <td> 0.000</td> <td>    3.923</td> <td>    4.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.0145</td> <td>    0.072</td> <td>   14.005</td> <td> 0.000</td> <td>    0.871</td> <td>    1.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    3.0579</td> <td>    0.045</td> <td>   67.296</td> <td> 0.000</td> <td>    2.968</td> <td>    3.148</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.402</td> <th>  Durbin-Watson:     </th> <td>   2.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.496</td> <th>  Jarque-Bera (JB):  </th> <td>   1.313</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.152</td> <th>  Prob(JB):          </th> <td>   0.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.527</td> <th>  Cond. No.          </th> <td>1.32e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 1.91e-30. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.826   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.825   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     466.2   \\\\\n",
       "\\textbf{Date:}             & Tue, 13 Feb 2024 & \\textbf{  Prob (F-statistic):} &  4.98e-39   \\\\\n",
       "\\textbf{Time:}             &     09:50:43     & \\textbf{  Log-Likelihood:    } &   -141.90   \\\\\n",
       "\\textbf{No. Observations:} &         100      & \\textbf{  AIC:               } &     287.8   \\\\\n",
       "\\textbf{Df Residuals:}     &          98      & \\textbf{  BIC:               } &     293.0   \\\\\n",
       "\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &       4.0724  &        0.075     &    54.100  &         0.000        &        3.923    &        4.222     \\\\\n",
       "\\textbf{x1}    &       1.0145  &        0.072     &    14.005  &         0.000        &        0.871    &        1.158     \\\\\n",
       "\\textbf{x2}    &       3.0579  &        0.045     &    67.296  &         0.000        &        2.968    &        3.148     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  1.402 & \\textbf{  Durbin-Watson:     } &    2.156  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.496 & \\textbf{  Jarque-Bera (JB):  } &    1.313  \\\\\n",
       "\\textbf{Skew:}          &  0.152 & \\textbf{  Prob(JB):          } &    0.519  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.527 & \\textbf{  Cond. No.          } & 1.32e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The smallest eigenvalue is 1.91e-30. This might indicate that there are \\newline\n",
       " strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.826\n",
       "Model:                            OLS   Adj. R-squared:                  0.825\n",
       "Method:                 Least Squares   F-statistic:                     466.2\n",
       "Date:                Tue, 13 Feb 2024   Prob (F-statistic):           4.98e-39\n",
       "Time:                        09:50:43   Log-Likelihood:                -141.90\n",
       "No. Observations:                 100   AIC:                             287.8\n",
       "Df Residuals:                      98   BIC:                             293.0\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          4.0724      0.075     54.100      0.000       3.923       4.222\n",
       "x1             1.0145      0.072     14.005      0.000       0.871       1.158\n",
       "x2             3.0579      0.045     67.296      0.000       2.968       3.148\n",
       "==============================================================================\n",
       "Omnibus:                        1.402   Durbin-Watson:                   2.156\n",
       "Prob(Omnibus):                  0.496   Jarque-Bera (JB):                1.313\n",
       "Skew:                           0.152   Prob(JB):                        0.519\n",
       "Kurtosis:                       2.527   Cond. No.                     1.32e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.91e-30. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Multicolinearity (#2)\n",
    "- We randomly generate 100 correlated X1 and X2 following a bi-variate normal distirbution and let X1 and X2 have different degree of correlation (0, 0.3, 0.6, 0.9, 0.95, and 0.99).\n",
    "\n",
    "- We speficy a true regression line: $y$ = 2*$X_1$ + 4*$X_2$ + e\n",
    "\n",
    "- We include X1 and X2 in our regression model: y ~ X1 + X2 (no intercept, for simplicity)\n",
    "\n",
    "- For each pre-set correlation value, we repeat 1000 times to generate the sampling distirbution of regression estimates.\n",
    "- Plot the sampling distirbutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a function to return the sampling distribution of regression coefficients \n",
    "# based on different degree of correlation between X1 and X2\n",
    "def simulation_multi_colinearity(cor):\n",
    "    params = []\n",
    "    for i in range(1000):\n",
    "        means = [2,2]\n",
    "        cov = [[1,cor], [cor,1]]\n",
    "        X = np.random.multivariate_normal(means,cov,100)\n",
    "        e = np.random.randn(100)*2\n",
    "        y = 2*X[:,0] + 4*X[:,1] + e\n",
    "        model = sm.OLS(y,X).fit()\n",
    "        params.append(model.params)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_dist_0 = simulation_multi_colinearity(0)\n",
    "sampling_dist_0_3 = simulation_multi_colinearity(0.3)\n",
    "sampling_dist_0_6 = simulation_multi_colinearity(0.6)\n",
    "sampling_dist_0_9 = simulation_multi_colinearity(0.9)\n",
    "sampling_dist_0_95 = simulation_multi_colinearity(0.95)\n",
    "sampling_dist_0_99 = simulation_multi_colinearity(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SE of estimates when X1 and X2 have a cor of 0: [0.14924138 0.15362695]\n",
      "SE of estimates when X1 and X2 have a cor of 0.3: [0.18184739 0.18149639]\n",
      "SE of estimates when X1 and X2 have a cor of 0.6: [0.23542159 0.23302496]\n",
      "SE of estimates when X1 and X2 have a cor of 0.9: [0.46005811 0.46522827]\n",
      "SE of estimates when X1 and X2 have a cor of 0.95: [0.62053723 0.61689469]\n",
      "SE of estimates when X1 and X2 have a cor of 0.99: [1.5058649  1.50165901]\n"
     ]
    }
   ],
   "source": [
    "print(\"SE of estimates when X1 and X2 have a cor of 0:\", np.std(sampling_dist_0,axis=0))\n",
    "print(\"SE of estimates when X1 and X2 have a cor of 0.3:\", np.std(sampling_dist_0_3,axis=0))\n",
    "print(\"SE of estimates when X1 and X2 have a cor of 0.6:\", np.std(sampling_dist_0_6,axis=0))\n",
    "print(\"SE of estimates when X1 and X2 have a cor of 0.9:\", np.std(sampling_dist_0_9,axis=0))\n",
    "print(\"SE of estimates when X1 and X2 have a cor of 0.95:\", np.std(sampling_dist_0_95,axis=0))\n",
    "print(\"SE of estimates when X1 and X2 have a cor of 0.99:\", np.std(sampling_dist_0_99,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of estimates when X1 and X2 have a cor of 0: [1.99528853 4.00220908]\n",
      "Mean of estimates when X1 and X2 have a cor of 0.3: [1.99374527 4.00339456]\n",
      "Mean of estimates when X1 and X2 have a cor of 0.6: [2.01461324 3.98911849]\n",
      "Mean of estimates when X1 and X2 have a cor of 0.9: [2.02487827 3.97569048]\n",
      "Mean of estimates when X1 and X2 have a cor of 0.95: [2.04199266 3.95822687]\n",
      "Mean of estimates when X1 and X2 have a cor of 0.99: [1.96516269 4.03487359]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean of estimates when X1 and X2 have a cor of 0:\", np.mean(sampling_dist_0,axis=0))\n",
    "print(\"Mean of estimates when X1 and X2 have a cor of 0.3:\", np.mean(sampling_dist_0_3,axis=0))\n",
    "print(\"Mean of estimates when X1 and X2 have a cor of 0.6:\", np.mean(sampling_dist_0_6,axis=0))\n",
    "print(\"Mean of estimates when X1 and X2 have a cor of 0.9:\", np.mean(sampling_dist_0_9,axis=0))\n",
    "print(\"Mean of estimates when X1 and X2 have a cor of 0.95:\", np.mean(sampling_dist_0_95,axis=0))\n",
    "print(\"Mean of estimates when X1 and X2 have a cor of 0.99:\", np.mean(sampling_dist_0_99,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that, with varying degrees of correlation between the two predictors in the model, the regression coefficients are still **unbiased**, but the standard error of the estimates are **inflated** based on the degree of the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding non-related predictors (#2)\n",
    "- We randomly generate 100 data points for X1, X2, and X3 following a normal distribution.\n",
    "\n",
    "- We speficy a true regression line: $y$ = 3 + 2*$X_1$ + 4*$X_2$ + e\n",
    "\n",
    "- We  include X1, X2, and X3in our regression model: y ~ X1 (with intercept). Here X3 should not be in the model, but included.\n",
    "\n",
    "- We fit 1000 such regression models and check for biasedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = []\n",
    "for i in range(1000):\n",
    "    X1 = np.random.randn(100)\n",
    "    X2 = np.random.randn(100)\n",
    "    X3 = np.random.randn(100)\n",
    "    e = np.random.randn(100)\n",
    "\n",
    "    y = 3 + 2*X1 + 4*X2 + e\n",
    "\n",
    "    X = sm.add_constant(np.column_stack([X1,X2,X3]))\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    estimates.append(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of estimates: [ 3.00132214e+00  1.99576168e+00  3.99886082e+00 -2.36659646e-03]\n",
      "SE of estimates: [0.10146959 0.09980508 0.10321398 0.1018764 ]\n"
     ]
    }
   ],
   "source": [
    "#The averages of estimates across the 1000 replications are:\n",
    "print(\"Average of estimates:\", np.mean(np.array(estimates),axis=0))\n",
    "print(\"SE of estimates:\", np.std(np.array(estimates),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the estimates of X1 and X2 are **unbiased**, and the standard errors are also quite small. The estimate for X3 is nearly **zero**, and if we check one model out of the 1000, we find the estimate is not statistically significant. In this sense, including non-related random data into the model does not do  much harm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.961</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   806.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 13 Feb 2024</td> <th>  Prob (F-statistic):</th> <td>6.27e-68</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:50:44</td>     <th>  Log-Likelihood:    </th> <td> -138.60</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   285.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    96</td>      <th>  BIC:               </th> <td>   295.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    3.0338</td> <td>    0.099</td> <td>   30.704</td> <td> 0.000</td> <td>    2.838</td> <td>    3.230</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.9591</td> <td>    0.094</td> <td>   20.781</td> <td> 0.000</td> <td>    1.772</td> <td>    2.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    3.9472</td> <td>    0.090</td> <td>   43.918</td> <td> 0.000</td> <td>    3.769</td> <td>    4.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.1564</td> <td>    0.099</td> <td>    1.580</td> <td> 0.117</td> <td>   -0.040</td> <td>    0.353</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.692</td> <th>  Durbin-Watson:     </th> <td>   2.382</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.260</td> <th>  Jarque-Bera (JB):  </th> <td>   2.525</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.387</td> <th>  Prob(JB):          </th> <td>   0.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.920</td> <th>  Cond. No.          </th> <td>    1.20</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.962   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.961   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     806.9   \\\\\n",
       "\\textbf{Date:}             & Tue, 13 Feb 2024 & \\textbf{  Prob (F-statistic):} &  6.27e-68   \\\\\n",
       "\\textbf{Time:}             &     09:50:44     & \\textbf{  Log-Likelihood:    } &   -138.60   \\\\\n",
       "\\textbf{No. Observations:} &         100      & \\textbf{  AIC:               } &     285.2   \\\\\n",
       "\\textbf{Df Residuals:}     &          96      & \\textbf{  BIC:               } &     295.6   \\\\\n",
       "\\textbf{Df Model:}         &           3      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &       3.0338  &        0.099     &    30.704  &         0.000        &        2.838    &        3.230     \\\\\n",
       "\\textbf{x1}    &       1.9591  &        0.094     &    20.781  &         0.000        &        1.772    &        2.146     \\\\\n",
       "\\textbf{x2}    &       3.9472  &        0.090     &    43.918  &         0.000        &        3.769    &        4.126     \\\\\n",
       "\\textbf{x3}    &       0.1564  &        0.099     &     1.580  &         0.117        &       -0.040    &        0.353     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  2.692 & \\textbf{  Durbin-Watson:     } &    2.382  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.260 & \\textbf{  Jarque-Bera (JB):  } &    2.525  \\\\\n",
       "\\textbf{Skew:}          & -0.387 & \\textbf{  Prob(JB):          } &    0.283  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.920 & \\textbf{  Cond. No.          } &     1.20  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.962\n",
       "Model:                            OLS   Adj. R-squared:                  0.961\n",
       "Method:                 Least Squares   F-statistic:                     806.9\n",
       "Date:                Tue, 13 Feb 2024   Prob (F-statistic):           6.27e-68\n",
       "Time:                        09:50:44   Log-Likelihood:                -138.60\n",
       "No. Observations:                 100   AIC:                             285.2\n",
       "Df Residuals:                      96   BIC:                             295.6\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          3.0338      0.099     30.704      0.000       2.838       3.230\n",
       "x1             1.9591      0.094     20.781      0.000       1.772       2.146\n",
       "x2             3.9472      0.090     43.918      0.000       3.769       4.126\n",
       "x3             0.1564      0.099      1.580      0.117      -0.040       0.353\n",
       "==============================================================================\n",
       "Omnibus:                        2.692   Durbin-Watson:                   2.382\n",
       "Prob(Omnibus):                  0.260   Jarque-Bera (JB):                2.525\n",
       "Skew:                          -0.387   Prob(JB):                        0.283\n",
       "Kurtosis:                       2.920   Cond. No.                         1.20\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
